# pocket-core
Native AI core for an offline mobile assistant using llama.cpp
# Offline AI Core

Native AI inference engine for an offline mobile assistant.

## Features
- Offline text chat (Gemma 1B)
- Offline image explanation (SmolVLM)
- llama.cpp based inference
- Android first
- One model loaded at a time

## Tech Stack
- C++
- llama.cpp
- GGUF models
- Android NDK

## Status
Under active development
